{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from  tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to tokenize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<00V>': 1, 'love': 2, 'i': 3, 'my': 4, 'you': 5, 'dog': 6, 'cat': 7, 'your': 8, 'do': 9, 'think': 10, 'is': 11, 'amazing': 12}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentences = ['I love my dog', 'I love my cat', 'You love your cat!', 'I love you.', 'Do you think my dog is amazing?']\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 100, oov_token='<00V>')  ## num_words parameter: maximum number of words to keep from a corpus (most frequent ones?)\n",
    "                                                           ## oov_token handles with 'out of vacabulary' tokens\n",
    "tokenizer.fit_on_texts(sentences)     ## goes through the corpus and looks for the most frequent words?\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turning sentences into data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 2, 4, 6], [3, 2, 4, 7], [5, 2, 8, 7], [3, 2, 5], [9, 5, 10, 4, 6, 11, 12]]\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 1, 2, 4, 6], [4, 6, 1, 4, 1]]\n"
     ]
    }
   ],
   "source": [
    "test_data = ['I really love my dog.', 'my dog loves my hammock']\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "print(test_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle sequences with different length one can use a RaggedTensor. Maybe look that up!\n",
    "Can also be handled with padding; simpler solution! Will for now do this here:\n",
    "\n",
    "(All sequences will have the same length as the longest, by padding it with 0's in the beginning. If you want them in the end set `pedding = 'post'` in `pad_sequences()`.\n",
    "\n",
    "Or put the maximum length of the sequences with `maxlen=5` etc.; with truncating you can specify whether words should be chopped of at the end or the beginning: `truncating='post'` or `'pre'`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  3  2  4  6]\n",
      " [ 0  0  0  3  2  4  7]\n",
      " [ 0  0  0  5  2  8  7]\n",
      " [ 0  0  0  0  3  2  5]\n",
      " [ 9  5 10  4  6 11 12]]\n"
     ]
    }
   ],
   "source": [
    "padded = pad_sequences(sequences)\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognize sentiment in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Sarcasm_Headlines_Dataset_v2.json', 'r') as f:\n",
    "    datastore = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "labels = []\n",
    "urls = []\n",
    "\n",
    "for item in datastore:\n",
    "    sentences.append(item['headline'])\n",
    "    labels.append(item['is_sarcastic'])\n",
    "    urls.append(item['article_link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 100\n",
    "trunc_type='post'\n",
    "#padding_type='post'\n",
    "#oov_tok = \"<OOV>\"\n",
    "#training_size = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences = sentences[0:22400]\n",
    "training_labels = labels[0:22400]\n",
    "\n",
    "testing_sentences = sentences[22400:]\n",
    "testing_labels = labels[22400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 3 2 4 6]\n",
      "(5, 7)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<00V>')  #num_words=vocab_size\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length,\n",
    "                                padding = 'post', truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, \n",
    "                               padding = 'post', truncating=trunc_type)\n",
    "\n",
    "print(padded[0])\n",
    "print(padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need this block to get it to work with TensorFlow 2.x\n",
    "training_padded = np.array(training_padded)\n",
    "training_labels = np.array(training_labels)\n",
    "testing_padded = np.array(testing_padded)\n",
    "testing_labels = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "700/700 - 3s - loss: 0.6385 - accuracy: 0.6491 - val_loss: 0.4908 - val_accuracy: 0.8177 - 3s/epoch - 4ms/step\n",
      "Epoch 2/30\n",
      "700/700 - 2s - loss: 0.3884 - accuracy: 0.8431 - val_loss: 0.3695 - val_accuracy: 0.8411 - 2s/epoch - 3ms/step\n",
      "Epoch 3/30\n",
      "700/700 - 2s - loss: 0.2963 - accuracy: 0.8832 - val_loss: 0.3497 - val_accuracy: 0.8492 - 2s/epoch - 3ms/step\n",
      "Epoch 4/30\n",
      "700/700 - 2s - loss: 0.2521 - accuracy: 0.9008 - val_loss: 0.3364 - val_accuracy: 0.8569 - 2s/epoch - 3ms/step\n",
      "Epoch 5/30\n",
      "700/700 - 2s - loss: 0.2177 - accuracy: 0.9159 - val_loss: 0.3481 - val_accuracy: 0.8492 - 2s/epoch - 3ms/step\n",
      "Epoch 6/30\n",
      "700/700 - 2s - loss: 0.1941 - accuracy: 0.9283 - val_loss: 0.3466 - val_accuracy: 0.8564 - 2s/epoch - 3ms/step\n",
      "Epoch 7/30\n",
      "700/700 - 2s - loss: 0.1739 - accuracy: 0.9352 - val_loss: 0.3589 - val_accuracy: 0.8558 - 2s/epoch - 3ms/step\n",
      "Epoch 8/30\n",
      "700/700 - 2s - loss: 0.1567 - accuracy: 0.9431 - val_loss: 0.3818 - val_accuracy: 0.8471 - 2s/epoch - 3ms/step\n",
      "Epoch 9/30\n",
      "700/700 - 2s - loss: 0.1426 - accuracy: 0.9493 - val_loss: 0.3957 - val_accuracy: 0.8485 - 2s/epoch - 3ms/step\n",
      "Epoch 10/30\n",
      "700/700 - 2s - loss: 0.1305 - accuracy: 0.9541 - val_loss: 0.4242 - val_accuracy: 0.8431 - 2s/epoch - 3ms/step\n",
      "Epoch 11/30\n",
      "700/700 - 2s - loss: 0.1187 - accuracy: 0.9610 - val_loss: 0.4528 - val_accuracy: 0.8386 - 2s/epoch - 3ms/step\n",
      "Epoch 12/30\n",
      "700/700 - 2s - loss: 0.1105 - accuracy: 0.9632 - val_loss: 0.4804 - val_accuracy: 0.8360 - 2s/epoch - 3ms/step\n",
      "Epoch 13/30\n",
      "700/700 - 2s - loss: 0.1024 - accuracy: 0.9669 - val_loss: 0.5074 - val_accuracy: 0.8333 - 2s/epoch - 3ms/step\n",
      "Epoch 14/30\n",
      "700/700 - 2s - loss: 0.0948 - accuracy: 0.9693 - val_loss: 0.5242 - val_accuracy: 0.8299 - 2s/epoch - 3ms/step\n",
      "Epoch 15/30\n",
      "700/700 - 2s - loss: 0.0883 - accuracy: 0.9710 - val_loss: 0.5752 - val_accuracy: 0.8257 - 2s/epoch - 3ms/step\n",
      "Epoch 16/30\n",
      "700/700 - 2s - loss: 0.0818 - accuracy: 0.9742 - val_loss: 0.5877 - val_accuracy: 0.8297 - 2s/epoch - 3ms/step\n",
      "Epoch 17/30\n",
      "700/700 - 2s - loss: 0.0771 - accuracy: 0.9748 - val_loss: 0.6134 - val_accuracy: 0.8254 - 2s/epoch - 3ms/step\n",
      "Epoch 18/30\n",
      "700/700 - 2s - loss: 0.0725 - accuracy: 0.9767 - val_loss: 0.6426 - val_accuracy: 0.8218 - 2s/epoch - 3ms/step\n",
      "Epoch 19/30\n",
      "700/700 - 2s - loss: 0.0669 - accuracy: 0.9793 - val_loss: 0.7030 - val_accuracy: 0.8152 - 2s/epoch - 3ms/step\n",
      "Epoch 20/30\n",
      "700/700 - 2s - loss: 0.0661 - accuracy: 0.9789 - val_loss: 0.7325 - val_accuracy: 0.8178 - 2s/epoch - 3ms/step\n",
      "Epoch 21/30\n",
      "700/700 - 2s - loss: 0.0579 - accuracy: 0.9828 - val_loss: 0.7655 - val_accuracy: 0.8170 - 2s/epoch - 3ms/step\n",
      "Epoch 22/30\n",
      "700/700 - 2s - loss: 0.0544 - accuracy: 0.9831 - val_loss: 0.7861 - val_accuracy: 0.8173 - 2s/epoch - 3ms/step\n",
      "Epoch 23/30\n",
      "700/700 - 2s - loss: 0.0514 - accuracy: 0.9852 - val_loss: 0.8279 - val_accuracy: 0.8146 - 2s/epoch - 3ms/step\n",
      "Epoch 24/30\n",
      "700/700 - 2s - loss: 0.0473 - accuracy: 0.9862 - val_loss: 0.8921 - val_accuracy: 0.8128 - 2s/epoch - 3ms/step\n",
      "Epoch 25/30\n",
      "700/700 - 2s - loss: 0.0457 - accuracy: 0.9862 - val_loss: 0.9329 - val_accuracy: 0.8127 - 2s/epoch - 3ms/step\n",
      "Epoch 26/30\n",
      "700/700 - 2s - loss: 0.0444 - accuracy: 0.9868 - val_loss: 0.9340 - val_accuracy: 0.8106 - 2s/epoch - 3ms/step\n",
      "Epoch 27/30\n",
      "700/700 - 2s - loss: 0.0412 - accuracy: 0.9880 - val_loss: 0.9549 - val_accuracy: 0.8066 - 2s/epoch - 3ms/step\n",
      "Epoch 28/30\n",
      "700/700 - 2s - loss: 0.0372 - accuracy: 0.9906 - val_loss: 1.0295 - val_accuracy: 0.8088 - 2s/epoch - 3ms/step\n",
      "Epoch 29/30\n",
      "700/700 - 2s - loss: 0.0361 - accuracy: 0.9899 - val_loss: 1.1136 - val_accuracy: 0.8069 - 2s/epoch - 3ms/step\n",
      "Epoch 30/30\n",
      "700/700 - 2s - loss: 0.0349 - accuracy: 0.9893 - val_loss: 1.1072 - val_accuracy: 0.8070 - 2s/epoch - 3ms/step\n"
     ]
    }
   ],
   "source": [
    "num_epochs=30\n",
    "\n",
    "history = model.fit(training_padded, training_labels, epochs=num_epochs,\n",
    "                    validation_data=(testing_padded, testing_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 119ms/step\n",
      "[[0.20531791]\n",
      " [0.00509081]]\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"granny starting to fear spiders in the garden might be real\", \"game of thrones season finale showing this sunday night\"]\n",
    "sequences = tokenizer.texts_to_sequences(sentence)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, padding='post', truncating=trunc_type)\n",
    "print(model.predict(padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try first AI creating poetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('mary_oliver.csv')\n",
    "poem = data.iloc[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'there is a thing in me that dreamed of trees,',\n",
       " 'a quiet house, some green and modest acres',\n",
       " 'a little way from every troubling town,',\n",
       " 'a little way from factories, schools, laments.',\n",
       " 'i would have time, i thought, and time to spare,',\n",
       " 'with only streams and birds for company,',\n",
       " 'to build out of my life a few wild stanzas.',\n",
       " 'and then it came to me, that so was death,',\n",
       " 'a little way away from everywhere.',\n",
       " 'there is a thing in me still dreams of trees.',\n",
       " 'but let it go. homesick for moderation,',\n",
       " \"half the world's artists shrink or fall away.\",\n",
       " 'if any find solution, let him tell it.',\n",
       " 'meanwhile i bend my heart toward lamentation',\n",
       " 'where, as the times implore our true involvement,',\n",
       " 'the blades of every crisis point the way.',\n",
       " 'i would it were not so, but so it is.',\n",
       " 'who ever made music of a mild day? ']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = poem.lower().split('\\n')\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]     ## This creates for each line a series of sequences with\n",
    "                                               ## only parts of the words: word 1 and 2 for the first\n",
    "                                               ## sequence, word 1, 2 and 3 for the next and so on\n",
    "        input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pad the input_sequences:\n",
    "\n",
    "max_sequence_len = max([len(x) for x in input_sequences])   ## Find the maximum length of the sequences \n",
    "padding_style='pre'\n",
    "\n",
    "input_sequences = np.array(pad_sequences(input_sequences, \n",
    "                                         maxlen=max_sequence_len, \n",
    "                                         padding=padding_style))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split into labels and X\n",
    "\n",
    "xs = input_sequences[:, :-1]\n",
    "labels = input_sequences[:, -1]\n",
    "\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words) ## so we can one-hot-encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_for_1hot = labels.reshape(-1,1)  ## This was suggested from an error message, because one-hot-encoder is expecting a 2D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## One-Hot-Encode:\n",
    "\n",
    "#Fit the encoder and get the columns from cat_data:\n",
    "encoder = OneHotEncoder().fit(labels_for_1hot)\n",
    "\n",
    "#Transform the categorical data with the encoder and put it into a DataFrame\n",
    "encoded = encoder.transform(labels_for_1hot).toarray()\n",
    "#encoded_df = pd.DataFrame(encoded,columns=cols)\n",
    "\n",
    "\n",
    "display(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 10s 52ms/step - loss: 4.6728 - accuracy: 0.0153\n",
      "Epoch 2/20\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 4.3709 - accuracy: 0.0916\n",
      "Epoch 3/20\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 3.7682 - accuracy: 0.1145\n",
      "Epoch 4/20\n",
      "5/5 [==============================] - 0s 42ms/step - loss: 3.1058 - accuracy: 0.1756\n",
      "Epoch 5/20\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 2.5085 - accuracy: 0.2824\n",
      "Epoch 6/20\n",
      "5/5 [==============================] - 0s 50ms/step - loss: 2.1125 - accuracy: 0.3893\n",
      "Epoch 7/20\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 1.7299 - accuracy: 0.4122\n",
      "Epoch 8/20\n",
      "5/5 [==============================] - 0s 46ms/step - loss: 1.3360 - accuracy: 0.5878\n",
      "Epoch 9/20\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.9402 - accuracy: 0.6870\n",
      "Epoch 10/20\n",
      "5/5 [==============================] - 0s 50ms/step - loss: 0.8484 - accuracy: 0.7557\n",
      "Epoch 11/20\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 0.6774 - accuracy: 0.8321\n",
      "Epoch 12/20\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.5783 - accuracy: 0.8321\n",
      "Epoch 13/20\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.5144 - accuracy: 0.8626\n",
      "Epoch 14/20\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 0.4338 - accuracy: 0.9008\n",
      "Epoch 15/20\n",
      "5/5 [==============================] - 0s 44ms/step - loss: 0.3175 - accuracy: 0.9160\n",
      "Epoch 16/20\n",
      "5/5 [==============================] - 0s 52ms/step - loss: 0.3170 - accuracy: 0.9008\n",
      "Epoch 17/20\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 0.2247 - accuracy: 0.9389\n",
      "Epoch 18/20\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 0.2212 - accuracy: 0.9237\n",
      "Epoch 19/20\n",
      "5/5 [==============================] - 0s 50ms/step - loss: 0.2264 - accuracy: 0.9237\n",
      "Epoch 20/20\n",
      "5/5 [==============================] - 0s 41ms/step - loss: 0.1654 - accuracy: 0.9618\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 240, input_length = max_sequence_len-1))\n",
    "model.add(Bidirectional(LSTM(150)))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "adam = Adam(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "history = model.fit(xs, ys, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-06af9a3d9d43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mtoken_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseed_text\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mtoken_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_sequence_len\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pre'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "seed_text = \"what is this\"\n",
    "next_words = 1\n",
    "  \n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "print(seed_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
