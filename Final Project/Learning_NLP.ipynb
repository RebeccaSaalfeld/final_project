{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from  tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to tokenize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<00V>': 1, 'love': 2, 'i': 3, 'my': 4, 'you': 5, 'dog': 6, 'cat': 7, 'your': 8, 'do': 9, 'think': 10, 'is': 11, 'amazing': 12}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentences = ['I love my dog', 'I love my cat', 'You love your cat!', 'I love you.', 'Do you think my dog is amazing?']\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 100, oov_token='<00V>')  ## num_words parameter: maximum number of words to keep from a corpus (most frequent ones?)\n",
    "                                                           ## oov_token handles with 'out of vacabulary' tokens\n",
    "tokenizer.fit_on_texts(sentences)     ## goes through the corpus and looks for the most frequent words?\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turning sentences into data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 2, 4, 6], [3, 2, 4, 7], [5, 2, 8, 7], [3, 2, 5], [9, 5, 10, 4, 6, 11, 12]]\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 1, 2, 4, 6], [4, 6, 1, 4, 1]]\n"
     ]
    }
   ],
   "source": [
    "test_data = ['I really love my dog.', 'my dog loves my hammock']\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "print(test_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle sequences with different length one can use a RaggedTensor. Maybe look that up!\n",
    "Can also be handled with padding; simpler solution! Will for now do this here:\n",
    "\n",
    "(All sequences will have the same length as the longest, by padding it with 0's in the beginning. If you want them in the end set `pedding = 'post'` in `pad_sequences()`.\n",
    "\n",
    "Or put the maximum length of the sequences with `maxlen=5` etc.; with truncating you can specify whether words should be chopped of at the end or the beginning: `truncating='post'` or `'pre'`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  3  2  4  6]\n",
      " [ 0  0  0  3  2  4  7]\n",
      " [ 0  0  0  5  2  8  7]\n",
      " [ 0  0  0  0  3  2  5]\n",
      " [ 9  5 10  4  6 11 12]]\n"
     ]
    }
   ],
   "source": [
    "padded = pad_sequences(sequences)\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognize sentiment in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Sarcasm_Headlines_Dataset_v2.json', 'r') as f:\n",
    "    datastore = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "labels = []\n",
    "urls = []\n",
    "\n",
    "for item in datastore:\n",
    "    sentences.append(item['headline'])\n",
    "    labels.append(item['is_sarcastic'])\n",
    "    urls.append(item['article_link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 100\n",
    "trunc_type='post'\n",
    "#padding_type='post'\n",
    "#oov_tok = \"<OOV>\"\n",
    "#training_size = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences = sentences[0:22400]\n",
    "training_labels = labels[0:22400]\n",
    "\n",
    "testing_sentences = sentences[22400:]\n",
    "testing_labels = labels[22400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16004   355  3167  7474  2644     3   661  1119     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "(28619, 152)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<00V>')  #num_words=vocab_size\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length,\n",
    "                                padding = 'post', truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, \n",
    "                               padding = 'post', truncating=trunc_type)\n",
    "\n",
    "print(padded[0])\n",
    "print(padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need this block to get it to work with TensorFlow 2.x\n",
    "training_padded = np.array(training_padded)\n",
    "training_labels = np.array(training_labels)\n",
    "testing_padded = np.array(testing_padded)\n",
    "testing_labels = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 16)           160000    \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 16)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 24)                408       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 25        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,433\n",
      "Trainable params: 160,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "700/700 - 3s - loss: 0.6415 - accuracy: 0.6371 - val_loss: 0.4875 - val_accuracy: 0.8144 - 3s/epoch - 4ms/step\n",
      "Epoch 2/30\n",
      "700/700 - 2s - loss: 0.3812 - accuracy: 0.8459 - val_loss: 0.3588 - val_accuracy: 0.8509 - 2s/epoch - 3ms/step\n",
      "Epoch 3/30\n",
      "700/700 - 2s - loss: 0.2927 - accuracy: 0.8809 - val_loss: 0.3387 - val_accuracy: 0.8550 - 2s/epoch - 3ms/step\n",
      "Epoch 4/30\n",
      "700/700 - 2s - loss: 0.2448 - accuracy: 0.9028 - val_loss: 0.3369 - val_accuracy: 0.8554 - 2s/epoch - 3ms/step\n",
      "Epoch 5/30\n",
      "700/700 - 2s - loss: 0.2132 - accuracy: 0.9158 - val_loss: 0.3445 - val_accuracy: 0.8532 - 2s/epoch - 3ms/step\n",
      "Epoch 6/30\n",
      "700/700 - 2s - loss: 0.1876 - accuracy: 0.9292 - val_loss: 0.3516 - val_accuracy: 0.8542 - 2s/epoch - 3ms/step\n",
      "Epoch 7/30\n",
      "700/700 - 2s - loss: 0.1674 - accuracy: 0.9375 - val_loss: 0.3843 - val_accuracy: 0.8434 - 2s/epoch - 3ms/step\n",
      "Epoch 8/30\n",
      "700/700 - 2s - loss: 0.1517 - accuracy: 0.9443 - val_loss: 0.3868 - val_accuracy: 0.8484 - 2s/epoch - 3ms/step\n",
      "Epoch 9/30\n",
      "700/700 - 2s - loss: 0.1359 - accuracy: 0.9513 - val_loss: 0.4146 - val_accuracy: 0.8455 - 2s/epoch - 3ms/step\n",
      "Epoch 10/30\n",
      "700/700 - 2s - loss: 0.1248 - accuracy: 0.9563 - val_loss: 0.4556 - val_accuracy: 0.8382 - 2s/epoch - 3ms/step\n",
      "Epoch 11/30\n",
      "700/700 - 2s - loss: 0.1129 - accuracy: 0.9637 - val_loss: 0.4626 - val_accuracy: 0.8378 - 2s/epoch - 3ms/step\n",
      "Epoch 12/30\n",
      "700/700 - 2s - loss: 0.1049 - accuracy: 0.9648 - val_loss: 0.4935 - val_accuracy: 0.8366 - 2s/epoch - 3ms/step\n",
      "Epoch 13/30\n",
      "700/700 - 2s - loss: 0.0958 - accuracy: 0.9667 - val_loss: 0.5245 - val_accuracy: 0.8334 - 2s/epoch - 3ms/step\n",
      "Epoch 14/30\n",
      "700/700 - 2s - loss: 0.0883 - accuracy: 0.9708 - val_loss: 0.5855 - val_accuracy: 0.8249 - 2s/epoch - 3ms/step\n",
      "Epoch 15/30\n",
      "700/700 - 2s - loss: 0.0830 - accuracy: 0.9726 - val_loss: 0.5909 - val_accuracy: 0.8267 - 2s/epoch - 3ms/step\n",
      "Epoch 16/30\n",
      "700/700 - 2s - loss: 0.0763 - accuracy: 0.9747 - val_loss: 0.6311 - val_accuracy: 0.8226 - 2s/epoch - 3ms/step\n",
      "Epoch 17/30\n",
      "700/700 - 2s - loss: 0.0718 - accuracy: 0.9767 - val_loss: 0.6552 - val_accuracy: 0.8238 - 2s/epoch - 3ms/step\n",
      "Epoch 18/30\n",
      "700/700 - 2s - loss: 0.0672 - accuracy: 0.9782 - val_loss: 0.7121 - val_accuracy: 0.8188 - 2s/epoch - 3ms/step\n",
      "Epoch 19/30\n",
      "700/700 - 2s - loss: 0.0621 - accuracy: 0.9804 - val_loss: 0.7363 - val_accuracy: 0.8201 - 2s/epoch - 3ms/step\n",
      "Epoch 20/30\n",
      "700/700 - 2s - loss: 0.0561 - accuracy: 0.9835 - val_loss: 0.7698 - val_accuracy: 0.8173 - 2s/epoch - 3ms/step\n",
      "Epoch 21/30\n",
      "700/700 - 2s - loss: 0.0534 - accuracy: 0.9832 - val_loss: 0.8328 - val_accuracy: 0.8148 - 2s/epoch - 3ms/step\n",
      "Epoch 22/30\n",
      "700/700 - 2s - loss: 0.0519 - accuracy: 0.9840 - val_loss: 0.8482 - val_accuracy: 0.8160 - 2s/epoch - 3ms/step\n",
      "Epoch 23/30\n",
      "700/700 - 2s - loss: 0.0476 - accuracy: 0.9842 - val_loss: 0.8674 - val_accuracy: 0.8122 - 2s/epoch - 3ms/step\n",
      "Epoch 24/30\n",
      "700/700 - 2s - loss: 0.0424 - accuracy: 0.9867 - val_loss: 0.9201 - val_accuracy: 0.8096 - 2s/epoch - 3ms/step\n",
      "Epoch 25/30\n",
      "700/700 - 2s - loss: 0.0402 - accuracy: 0.9887 - val_loss: 0.9592 - val_accuracy: 0.8062 - 2s/epoch - 3ms/step\n",
      "Epoch 26/30\n",
      "700/700 - 2s - loss: 0.0380 - accuracy: 0.9890 - val_loss: 0.9966 - val_accuracy: 0.8072 - 2s/epoch - 3ms/step\n",
      "Epoch 27/30\n",
      "700/700 - 2s - loss: 0.0353 - accuracy: 0.9896 - val_loss: 1.0425 - val_accuracy: 0.8099 - 2s/epoch - 3ms/step\n",
      "Epoch 28/30\n",
      "700/700 - 2s - loss: 0.0344 - accuracy: 0.9898 - val_loss: 1.0963 - val_accuracy: 0.8072 - 2s/epoch - 3ms/step\n",
      "Epoch 29/30\n",
      "700/700 - 2s - loss: 0.0298 - accuracy: 0.9914 - val_loss: 1.1239 - val_accuracy: 0.8025 - 2s/epoch - 3ms/step\n",
      "Epoch 30/30\n",
      "700/700 - 2s - loss: 0.0290 - accuracy: 0.9922 - val_loss: 1.1737 - val_accuracy: 0.7987 - 2s/epoch - 3ms/step\n"
     ]
    }
   ],
   "source": [
    "num_epochs=30\n",
    "\n",
    "history = model.fit(training_padded, training_labels, epochs=num_epochs,\n",
    "                    validation_data=(testing_padded, testing_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 120ms/step\n",
      "[[0.93781376]\n",
      " [0.00148034]]\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"granny starting to fear spiders in the garden might be real\", \"game of thrones season finale showing this sunday night\"]\n",
    "sequences = tokenizer.texts_to_sequences(sentence)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, padding='post', truncating=trunc_type)\n",
    "print(model.predict(padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try first AI creating poetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('mary_oliver.csv')\n",
    "poem = data.iloc[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'there is a thing in me that dreamed of trees,',\n",
       " 'a quiet house, some green and modest acres',\n",
       " 'a little way from every troubling town,',\n",
       " 'a little way from factories, schools, laments.',\n",
       " 'i would have time, i thought, and time to spare,',\n",
       " 'with only streams and birds for company,',\n",
       " 'to build out of my life a few wild stanzas.',\n",
       " 'and then it came to me, that so was death,',\n",
       " 'a little way away from everywhere.',\n",
       " 'there is a thing in me still dreams of trees.',\n",
       " 'but let it go. homesick for moderation,',\n",
       " \"half the world's artists shrink or fall away.\",\n",
       " 'if any find solution, let him tell it.',\n",
       " 'meanwhile i bend my heart toward lamentation',\n",
       " 'where, as the times implore our true involvement,',\n",
       " 'the blades of every crisis point the way.',\n",
       " 'i would it were not so, but so it is.',\n",
       " 'who ever made music of a mild day? ']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = poem.lower().split('\\n')\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]     ## This creates for each line a series of sequences with\n",
    "                                               ## only parts of the words: word 1 and 2 for the first\n",
    "                                               ## sequence, word 1, 2 and 3 for the next and so on\n",
    "        input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pad the input_sequences:\n",
    "\n",
    "max_sequence_len = max([len(x) for x in input_sequences])   ## Find the maximum length of the sequences \n",
    "padding_style='pre'\n",
    "\n",
    "input_sequences = np.array(pad_sequences(input_sequences, \n",
    "                                         maxlen=max_sequence_len, \n",
    "                                         padding=padding_style))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split into labels and X\n",
    "\n",
    "xs = input_sequences[:, :-1]\n",
    "labels = input_sequences[:, -1]\n",
    "\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words) ## so we can one-hot-encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One-Hot-Encode:\n",
    "\n",
    "#Fit the encoder and get the columns from cat_data:\n",
    "encoder = OneHotEncoder(drop='first').fit(cat_data)\n",
    "cols = encoder.get_feature_names(input_features=cat_data.columns) \n",
    "\n",
    "#Transform the categorical data with the encoder and put it into a DataFrame\n",
    "encoded = encoder.transform(cat_data).toarray()\n",
    "encoded_df = pd.DataFrame(encoded,columns=cols)\n",
    "\n",
    "\n",
    "display(encoded_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
